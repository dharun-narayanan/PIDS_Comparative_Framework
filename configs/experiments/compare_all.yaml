# Comparative Experiment Configuration
# Compare all PIDS models on the same dataset

experiment_name: compare_all_models
experiment_type: comparison

# Models to compare
models:
  - name: magic
    config: configs/models/magic.yaml
    variants:
      - magic
      - magic_streamspot
      - magic_darpa
      
  - name: kairos
    config: configs/models/kairos.yaml
    
  - name: orthrus
    config: configs/models/orthrus.yaml
    
  - name: threatrace
    config: configs/models/threatrace.yaml

# Dataset configuration
dataset:
  name: custom_soc         # custom_soc, cadets_e3, streamspot
  config: configs/datasets/custom_soc.yaml
  
# Training configuration
training:
  # Shared training settings
  max_epochs: 200
  early_stopping_patience: 30
  
  # Resource allocation
  device: cuda
  num_workers: 4
  pin_memory: true
  
  # Reproducibility
  seed: 42
  deterministic: true
  
  # Logging
  log_interval: 10         # Log every N batches
  save_interval: 10        # Save checkpoint every N epochs
  
# Evaluation configuration
evaluation:
  # Metrics to compute
  metrics:
    - auroc
    - auprc
    - f1_score
    - precision
    - recall
    - accuracy
    - detection_rate
    - false_positive_rate
    
  # Evaluation modes
  modes:
    - test                 # Standard test set evaluation
    - cross_validation     # Optional k-fold CV
    
  # Cross-validation settings
  cross_validation:
    enabled: false
    n_folds: 5
    
  # Anomaly detection thresholds
  threshold_search:
    enabled: true
    method: grid           # grid or bayesian
    percentiles: [90, 95, 97, 99]
    
# Results configuration
results:
  # Output directory
  output_dir: results/comparison/
  
  # Report generation
  generate_report: true
  report_format: html      # html, pdf, or markdown
  
  # Visualizations
  generate_plots: true
  plot_types:
    - roc_curve
    - precision_recall_curve
    - confusion_matrix
    - training_curves
    - model_comparison_bar
    - detection_timeline
    
  # Statistical analysis
  statistical_tests:
    enabled: true
    tests:
      - t_test            # Paired t-test
      - wilcoxon          # Wilcoxon signed-rank test
    confidence_level: 0.95
    
# Checkpointing
checkpointing:
  save_dir: checkpoints/comparison/
  save_best_per_model: true
  metric: auroc
  mode: max
  
# Performance profiling
profiling:
  enabled: true
  metrics:
    - training_time
    - inference_time
    - memory_usage
    - throughput
    
# Reproducibility
reproducibility:
  save_config: true
  save_environment: true   # Save Python packages
  save_random_states: true
  
# Notification (optional)
notification:
  enabled: false
  method: email           # email or slack
  on_completion: true
  on_error: true
