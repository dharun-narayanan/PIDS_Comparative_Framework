# PIDS Framework - Complete Setup Guide

## üéØ Overview

This framework evaluates **5 pretrained PIDS models** on your custom SOC data:
- **MAGIC** - Masked Graph Autoencoder
- **Kairos** - Temporal GNN
- **Orthrus** - Multi-Decoder Contrastive Learning
- **ThreaTrace** - Scalable Graph Processing
- **Continuum_FL** - Federated Learning PIDS

**Default workflow**: Setup environment ‚Üí Setup models & weights ‚Üí Preprocess data ‚Üí Evaluate models ‚Üí Compare performance

---

## üìã Prerequisites

Before starting, ensure you have:
- ‚úÖ **Conda** installed (Anaconda or Miniconda)
- ‚úÖ **Python 3.8+** (will be installed via conda)
- ‚úÖ **Graphviz (system binary)** - required for visualization (optional but recommended). 
On macOS:
```bash
# Install Graphviz via Homebrew
brew install graphviz
# Verify
dot -V
```
On Debian/Ubuntu:
```bash
sudo apt-get update && sudo apt-get install -y graphviz
```
- ‚úÖ **Git** (for cloning repositories)
- ‚úÖ **50GB+** free disk space
- ‚úÖ **16GB+** RAM (8GB minimum)
- ‚öôÔ∏è **GPU** (optional - framework runs on CPU by default)

**Check Conda installation:**
```bash
conda --version
# If not installed, download from: https://docs.conda.io/en/latest/miniconda.html
```

---

## üöÄ Streamlined Setup (3 Simple Steps)

The framework has been optimized to minimize setup steps. You now need only **3 commands** to get started!

### Step 1: Navigate to Framework Directory

```bash
# Navigate to the framework
cd ../PIDS_Comparative_Framework

# Verify you're in the correct directory
ls
# Expected output: README.md, setup.md, scripts/, models/, etc.
```

---

### Step 2: Run Complete Environment Setup

**This single script does everything:**
- ‚úÖ Creates conda environment
- ‚úÖ Installs all core dependencies
- ‚úÖ Applies PyTorch MKL fix automatically
- ‚úÖ Creates directory structure
- ‚úÖ Verifies installation

```bash
# Make setup script executable (if not already)
chmod +x scripts/setup.sh

# Run the complete setup script
./scripts/setup.sh
```

**What this does:**
1. ‚úÖ Checks for Conda installation
2. ‚úÖ Creates `pids_framework` conda environment (Python 3.10)
3. ‚úÖ Installs PyTorch 1.12.1 with CUDA 11.6
4. ‚úÖ Installs DGL 1.0.0 (Deep Graph Library)
5. ‚úÖ Installs PyTorch Geometric 2.1.0
6. ‚úÖ Applies MKL threading fix automatically (no separate step needed!)
7. ‚úÖ Creates necessary directories
8. ‚úÖ Verifies installation

**Expected output:**
```
Step 1/7: Checking for Conda installation...
‚úì Conda is installed: conda 23.x.x

Step 2/7: Creating pids_framework environment...
‚úì Environment created successfully

Step 3/7: Initializing Conda for shell...
‚úì Conda initialized

Step 4/7: Activating environment...
‚úì Environment activated: pids_framework

Step 5/7: Applying PyTorch MKL threading fix...
‚úì MKL threading fix applied (will auto-activate with environment)
‚úì PyTorch is working!

Step 6/7: Creating directory structure...
‚úì Directory structure created

Step 7/7: Verifying installation...
‚úì All core dependencies installed successfully

============================================
‚úì Setup completed successfully!
============================================
```

**If setup fails**, the script includes automatic fixes for common issues. See [Troubleshooting](#troubleshooting) section if problems persist.

---

### Step 3: Activate Conda Environment

```bash
# Activate the environment
conda activate pids_framework

# Verify activation
echo $CONDA_DEFAULT_ENV
# Should output: pids_framework
```

**Important:** You must activate this environment every time you use the framework.

---

### Step 4: Setup Models and Pretrained Weights

**This unified script handles everything:**
- ‚úÖ Installs model-specific dependencies
- ‚úÖ **Downloads pretrained weights from GitHub repositories** (primary method)
- ‚úÖ Falls back to local directories if GitHub download fails

```bash
# Setup ALL models (recommended - downloads from GitHub!)
python scripts/setup_models.py --all

# OR setup specific models only
python scripts/setup_models.py --models magic kairos orthrus

# List available models and their GitHub repos
python scripts/setup_models.py --list
```

**How it works:**
1. **Primary Method**: Downloads weights directly from official GitHub repositories:
   - **MAGIC**: https://github.com/FDUDSDE/MAGIC
   - **Continuum_FL**: https://github.com/kamelferrahi/Continuum_FL
   - **ThreaTrace**: https://github.com/threaTrace-detector/threaTrace
   - **Orthrus**: https://github.com/ubc-provenance/orthrus
   - **Kairos**: https://github.com/ubc-provenance/kairos

2. **Fallback Method**: If GitHub download fails, searches local directories:
   - `../MAGIC/checkpoints/`
   - `../Continuum_FL/checkpoints/`
   - `../orthrus/weights/`
   - `../kairos/DARPA/`
   - `../threaTrace/example_models/`

**Expected output:**
```
================================================================================
  PIDS Framework - Model Setup (GitHub Download)
================================================================================
Setting up models: magic, kairos, orthrus, threatrace, continuum_fl
Strategy: Download from GitHub ‚Üí Fallback to local if needed

================================================================================
  Setting up MAGIC
================================================================================
Description: Masked Graph Autoencoder for APT Detection
GitHub: https://github.com/FDUDSDE/MAGIC

Installing dependencies for MAGIC...
‚úì MAGIC dependencies installed

Downloading weights for MAGIC...
   Repository: https://github.com/FDUDSDE/MAGIC
   üì• streamspot: MAGIC trained on StreamSpot dataset
   Downloading with curl: checkpoint-streamspot.pt
   ‚úì Downloaded: checkpoint-streamspot.pt
   üì• cadets: MAGIC trained on DARPA CADETS
   ‚úì Downloaded: checkpoint-cadets.pt
   ...
‚úì Downloaded 5 checkpoint(s) from GitHub/official sources

...

================================================================================
  Setup Summary
================================================================================
‚úì Dependencies installed for 5 model(s)
‚úì Downloaded 18 checkpoint(s) from GitHub/official sources
‚úì Copied 2 checkpoint(s) from local fallback

Checkpoints saved to: checkpoints/

Next steps:
  1. Verify weights: ls -lh checkpoints/*/
  2. Preprocess data: python scripts/preprocess_data.py
  3. Run evaluation: ./scripts/run_evaluation.sh
```

**Note on Special Cases:**
- **Kairos**: Pre-trained weights available from [Google Drive](https://drive.google.com/drive/folders/1YAKoO3G32xlYrCs4BuATt1h_hBvvEB6C) (manual download required)
- **Orthrus**: Pre-trained weights available from [Zenodo](https://zenodo.org/records/14641605) (automatic download via curl/wget)

The script will automatically use `curl` or `wget` (whichever is available) to download weights.

---

### Step 5: Prepare Your Custom Dataset

Place your SOC data in the `custom_dataset` directory (sibling to framework).

#### 6.1: Create Custom Dataset Directory

```bash
# Navigate to parent directory
cd ..

# Create custom_dataset directory if it doesn't exist
mkdir -p custom_dataset

# Copy your SOC logs
cp /path/to/your/logs/*.json custom_dataset/

# Verify data is present
ls custom_dataset/
# Expected: endpoint_file.json, endpoint_network.json, endpoint_process.json (or similar)

# Return to framework directory
cd PIDS_Comparative_Framework
```

#### 6.2: Data Format Requirements

Your JSON files should contain provenance events with:
- **Process events**: process creation, execution, termination
- **File events**: read, write, create, delete
- **Network events**: connect, bind, send, receive

**Example JSON format:**
```json
{
  "events": [
    {
      "timestamp": "2025-10-13T10:30:00Z",
      "event_type": "process_create",
      "pid": 1234,
      "ppid": 1000,
      "cmdline": "/bin/bash script.sh",
      "user": "admin"
    },
    {
      "timestamp": "2025-10-13T10:30:01Z",
      "event_type": "file_read",
      "pid": 1234,
      "file_path": "/etc/passwd",
      "operation": "read"
    }
  ]
}
```

#### 6.3: Download Benchmark Datasets (Optional)

If you want to test on standard benchmarks:

**DARPA TC Dataset:**
```bash
# Create data directory
mkdir -p data/darpa

# Download DARPA CADETS E3 (example - adjust URL)
# Note: DARPA datasets are large (10GB+) and require access approval
# Visit: https://github.com/darpa-i2o/Transparent-Computing
```

**StreamSpot Dataset:**
```bash
# Create data directory
mkdir -p data/streamspot

# Clone StreamSpot repository
git clone https://github.com/sbustreamspot/sbustreamspot-data.git data/streamspot
```

---

### Step 7: Verify Installation

Run the verification script to ensure everything is set up correctly.

```bash
# Run verification
python scripts/verify_installation.py

# Expected output:
# ‚úì Python version: 3.10.x
# ‚úì PyTorch installed: 1.12.1
# ‚úì DGL installed: 1.0.0
# ‚úì PyTorch Geometric installed: 2.1.0
# ‚úì All 9 models registered
# ‚úì Checkpoints directory exists
# ‚úì Configuration files present
# Installation verification complete!
```

**If any checks fail**, see the [Troubleshooting](#troubleshooting) section.

---

### Step 8: Run Evaluation (Primary Workflow)

Now you're ready to evaluate pretrained models on your data!

```bash
# Evaluate ALL models on your custom data (CPU by default)
./scripts/run_evaluation.sh

# Expected output:
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#     PIDS Comparative Framework - Evaluation Workflow
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 
# Configuration:
#   Model(s):      all
#   Dataset:       custom_soc
#   Data Path:     ../custom_dataset
#   Output Dir:    results/evaluation_20251013_103000
#
# Step 1/4: Downloading Pretrained Weights
# ‚úì Weights downloaded successfully
#
# Step 2/4: Preprocessing Custom Dataset
# ‚úì Data preprocessing completed
#
# Step 3/4: Running Model Evaluation
# Evaluating magic...
# ‚úì magic evaluation completed
# Evaluating kairos...
# ‚úì kairos evaluation completed
# ...
#
# Step 4/4: Generating Comparison Report
# ‚úì Comparison report generated
#
# ‚úì EVALUATION COMPLETED SUCCESSFULLY!
#
# Results saved to: results/evaluation_20251013_103000
```

**Results location:** `results/evaluation_YYYYMMDD_HHMMSS/`

---

### Step 9: View Results

```bash
# Navigate to results directory
cd results/evaluation_YYYYMMDD_HHMMSS/

# View comparison report
cat comparison_report.json

# Example output:
# {
#   "dataset": "custom_soc",
#   "timestamp": "2025-10-13T10:30:00",
#   "models": {
#     "magic": {
#       "auc_roc": 0.9245,
#       "auc_pr": 0.8532,
#       "f1": 0.8710,
#       "precision": 0.8456,
#       "recall": 0.8973
#     },
#     "kairos": {
#       "auc_roc": 0.9156,
#       ...
#     }
#   }
# }

# Check individual model logs
tail -n 50 magic_evaluation.log
tail -n 50 kairos_evaluation.log

# View predictions (if saved)
head predictions.json
```

---

## üìä Understanding Your Results

### Key Metrics Explained

| Metric | Description | Good Value | What It Means |
|--------|-------------|------------|---------------|
| **AUROC** | Area Under ROC Curve | > 0.90 | How well model separates normal vs attack |
| **AUPRC** | Area Under Precision-Recall | > 0.80 | Performance on imbalanced data |
| **F1-Score** | Harmonic mean of precision & recall | > 0.85 | Overall detection accuracy |
| **Precision** | TP / (TP + FP) | > 0.80 | How many detections are correct |
| **Recall** | TP / (TP + FN) | > 0.85 | How many attacks are caught |
| **FPR** | False Positive Rate | < 0.10 | False alarm rate (lower is better) |

### Interpreting Results

**Excellent Performance (AUROC > 0.90)**
- ‚úÖ Model works well on your data
- ‚úÖ Deploy to production
- ‚úÖ Monitor performance over time

**Good Performance (AUROC 0.80-0.90)**
- ‚ö†Ô∏è Acceptable but room for improvement
- Consider threshold tuning
- May benefit from fine-tuning

**Poor Performance (AUROC < 0.80)**
- ‚ùå Model not suitable for your data
- Try different model
- Consider retraining on labeled data

---

## üîß Advanced Options

### Evaluate Specific Model

```bash
# MAGIC only (fastest evaluation)
./scripts/run_evaluation.sh --model magic

# Kairos only
./scripts/run_evaluation.sh --model kairos

# Continuum_FL only
./scripts/run_evaluation.sh --model continuum_fl
```

### Custom Data Path

```bash
# Evaluate on data from different location
./scripts/run_evaluation.sh --data-path /path/to/soc/logs
```

### Use GPU (if available)

```bash
# Check if GPU is available
python -c "import torch; print(f'GPU available: {torch.cuda.is_available()}')"

# Force GPU usage for evaluation
CUDA_VISIBLE_DEVICES=0 ./scripts/run_evaluation.sh

# Specify GPU device (if multiple GPUs)
CUDA_VISIBLE_DEVICES=1 ./scripts/run_evaluation.sh --model magic
```

### Skip Steps (Already Completed)

```bash
# Skip downloading weights (already downloaded)
./scripts/run_evaluation.sh --skip-download

# Skip preprocessing (data already preprocessed)
./scripts/run_evaluation.sh --skip-preprocess

# Skip both
./scripts/run_evaluation.sh --skip-download --skip-preprocess
```

### Custom Output Directory

```bash
# Specify where to save results
./scripts/run_evaluation.sh --output-dir results/my_evaluation

# Results will be in: results/my_evaluation/
```

---

## üîÑ Advanced Feature: Retrain Models (Optional)

> ‚ö†Ô∏è **Only needed if pretrained models achieve AUROC < 0.80 on your data**

### Prerequisites for Retraining

- Labeled attack data (ground truth)
- Sufficient data (thousands of events)
- Time (training can take hours)

### Step-by-Step Retraining

#### 1. Prepare Labeled Data

```bash
# Create ground truth file
cat > data/custom_soc/ground_truth.json << EOF
{
  "attacks": [
    {
      "start_time": "2025-10-13T08:00:00Z",
      "end_time": "2025-10-13T09:00:00Z",
      "attack_type": "apt",
      "entities": [1234, 1235, 1236]
    }
  ]
}
EOF
```

#### 2. Preprocess Data with Labels

```bash
# Preprocess with ground truth
python scripts/preprocess_data.py \
    --input-dir ../custom_dataset/ \
    --labels-file data/custom_soc/ground_truth.json \
    --output-dir data/custom_soc \
    --dataset-name custom_soc
```

#### 3. Train Model (CPU by default)

```bash
# Train MAGIC on custom data (CPU)
python experiments/train.py \
    --model magic \
    --dataset custom_soc \
    --data-path data/custom_soc \
    --epochs 100 \
    --batch-size 32 \
    --device -1 \
    --save-dir checkpoints

# Train on GPU (if available)
python experiments/train.py \
    --model magic \
    --dataset custom_soc \
    --data-path data/custom_soc \
    --epochs 100 \
    --batch-size 32 \
    --device 0
```

#### 4. Evaluate Retrained Model

```bash
# Evaluate the retrained model
python experiments/evaluate.py \
    --model magic \
    --checkpoint checkpoints/magic_custom_soc_best.pt \
    --dataset custom_soc \
    --data-path data/custom_soc \
    --device -1
```

#### 5. Compare Pretrained vs Retrained

```bash
# Compare performance
python experiments/compare.py \
    --checkpoints checkpoints/magic/checkpoint-cadets-e3.pt checkpoints/magic_custom_soc_best.pt \
    --labels "Pretrained" "Retrained" \
    --dataset custom_soc \
    --output-dir results/comparison
```

### Fine-tune Pretrained Model

```bash
# Fine-tune (transfer learning) instead of training from scratch
python experiments/train.py \
    --model magic \
    --pretrained checkpoints/magic/checkpoint-cadets-e3.pt \
    --dataset custom_soc \
    --data-path data/custom_soc \
    --epochs 50 \
    --learning-rate 0.0001 \
    --device -1
```

---

## üêõ Troubleshooting

### Issue 1: Conda Not Found

**Error:** `conda: command not found`

**Solution:**
```bash
# Install Miniconda
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh

# Or on macOS
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh
bash Miniconda3-latest-MacOSX-x86_64.sh

# Restart terminal and retry
```

### Issue 2: Environment Not Activated

**Error:** `conda environment 'pids_framework' not activated`

**Solution:**
```bash
# Activate environment
conda activate pids_framework

# Verify
echo $CONDA_DEFAULT_ENV  # Should show: pids_framework

# Add to ~/.bashrc or ~/.zshrc for auto-activation (optional)
echo "conda activate pids_framework" >> ~/.zshrc
```

### Issue 3: Data Path Does Not Exist

**Error:** `Data path does not exist: ../custom_dataset`

**Solution:**
```bash
# Create directory and add data
mkdir -p ../custom_dataset

# Copy your JSON logs
cp /path/to/your/logs/*.json ../custom_dataset/

# Verify
ls -la ../custom_dataset/
```

### Issue 4: Pretrained Weights Not Found

**Error:** `Pretrained weights not found for model: magic`

**Solution:**
```bash
# Option 1: Copy from existing checkpoints
python scripts/download_weights.py --copy-existing --all-models

# Option 2: Download from URLs
python scripts/download_weights.py --model magic

# Option 3: Manual copy
mkdir -p checkpoints/magic
cp ../MAGIC/checkpoints/*.pt checkpoints/magic/
cp ../Continuum_FL/checkpoints/*.pt checkpoints/continuum_fl/

# Verify
ls -la checkpoints/
```

### Issue 5: Out of Memory

**Error:** `RuntimeError: CUDA out of memory` or `MemoryError`

**Solution:**
```bash
# Solution 1: Use CPU (default, safer)
python experiments/train.py --device -1

# Solution 2: Reduce batch size
# Edit configs/models/MODEL.yaml:
training:
  batch_size: 8  # or even smaller: 4, 2, 1

# Solution 3: Process data in chunks
python scripts/preprocess_data.py --chunk-size 1000
```

### Issue 6: Import Errors

**Error:** `ModuleNotFoundError: No module named 'torch'`

**Solution:**
```bash
# Ensure environment is activated
conda activate pids_framework

# Reinstall PyTorch
conda install pytorch==1.12.1 torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge

# Verify
python -c "import torch; print(torch.__version__)"
```

### PyTorch MKL / "undefined symbol: iJIT_NotifyEvent" error

If you see an import error mentioning `iJIT_NotifyEvent` (or other MKL-related symbol errors), this is usually caused by a mismatch between PyTorch and Intel MKL threading libraries. The repository includes an automated fixer to resolve this quickly.

Preferred automated fix:

```bash
# Activate environment first
conda activate pids_framework

# Make the fix script executable and run it
chmod +x scripts/fix_pytorch_mkl.sh
./scripts/fix_pytorch_mkl.sh

# Re-run verification
python scripts/test_pytorch.py
```

Quick manual fix (applies to current session and can be made permanent):

```bash
# For current session
export MKL_THREADING_LAYER=GNU

# To persist for the conda env
conda activate pids_framework
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
echo 'export MKL_THREADING_LAYER=GNU' > $CONDA_PREFIX/etc/conda/activate.d/mkl_fix.sh
```

If the automated script and manual fix do not work, try reinstalling a compatible MKL version:

```bash
conda activate pids_framework
conda install "mkl<2024" -c conda-forge --force-reinstall -y
```

Verification:

```bash
python -c "import torch; print(f'PyTorch {torch.__version__}'); print('CUDA available:', torch.cuda.is_available())"
```

See `docs/PYTORCH_MKL_FIX.md` and `scripts/fix_pytorch_mkl.sh` for more details.

### Issue 7: Model Not Registered

**Error:** `KeyError: 'magic'` or `Model not found`

**Solution:**
```bash
# Reinstall model dependencies
./scripts/install_model_deps.sh --models magic

# Verify registration
python -c "from models import list_available_models; print(list_available_models())"

# Expected output: ['magic', 'kairos', 'orthrus', 'threatrace', 'continuum_fl', ...]
```

### Issue 8: Permission Denied

**Error:** `Permission denied: ./scripts/run_evaluation.sh`

**Solution:**
```bash
# Make scripts executable
chmod +x scripts/*.sh

# Retry
./scripts/run_evaluation.sh
```

---

## üìö Additional Resources

- **README.md** - Complete framework documentation
- **configs/** - Configuration files for models and datasets
- **scripts/** - All utility scripts with `--help` options
- **experiments/** - Training, evaluation, and comparison scripts

### Get Help

```bash
# Script help
./scripts/run_evaluation.sh --help
python scripts/download_weights.py --help
python scripts/preprocess_data.py --help
python experiments/train.py --help
python experiments/evaluate.py --help
python experiments/compare.py --help

# Model information
python -c "from models import list_available_models; print(list_available_models())"

# Check installation
python scripts/verify_installation.py
```

---

## üéØ Quick Reference

### Essential Commands

```bash
# 1. Setup (once)
./scripts/setup.sh
conda activate pids_framework

# 2. Install model dependencies (once)
./scripts/install_model_deps.sh --all

# 3. Download/copy weights (once)
python scripts/download_weights.py --copy-existing --all-models

# 4. Verify setup (once)
python scripts/verify_installation.py

# 5. Prepare data (once per dataset)
mkdir -p ../custom_dataset
cp /path/to/logs/*.json ../custom_dataset/

# 6. Run evaluation (main workflow)
./scripts/run_evaluation.sh

# 7. View results
cat results/evaluation_*/comparison_report.json
```

### Quick Workflow Summary

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  PIDS Framework Workflow                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  Setup:                                                  ‚îÇ
‚îÇ  1. ./scripts/setup.sh                                   ‚îÇ
‚îÇ  2. conda activate pids_framework                        ‚îÇ
‚îÇ  3. ./scripts/install_model_deps.sh --all                ‚îÇ
‚îÇ  4. python scripts/download_weights.py --copy-existing   ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  Prepare Data:                                           ‚îÇ
‚îÇ  5. mkdir -p ../custom_dataset                           ‚îÇ
‚îÇ  6. cp logs/*.json ../custom_dataset/                    ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  Evaluate:                                               ‚îÇ
‚îÇ  7. ./scripts/run_evaluation.sh                          ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  Results:                                                ‚îÇ
‚îÇ  8. cat results/evaluation_*/comparison_report.json      ‚îÇ
‚îÇ                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéâ Success Checklist

Before running evaluation, verify:

- ‚úÖ Conda environment `pids_framework` is activated
- ‚úÖ All model dependencies installed (`./scripts/install_model_deps.sh --all`)
- ‚úÖ Pretrained weights downloaded/copied to `checkpoints/`
- ‚úÖ Custom dataset present in `../custom_dataset/`
- ‚úÖ Verification script passes (`python scripts/verify_installation.py`)

If all checks pass, you're ready to run:
```bash
./scripts/run_evaluation.sh
```

---

**Need more help?** See [README.md](README.md) for complete documentation.
